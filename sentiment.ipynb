{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dany503/CFP_2019/blob/master/sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVtPr-dzgnOf",
        "colab_type": "code",
        "outputId": "f48b0ae0-09cf-4313-c29f-caff934d26bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIiSX16oiwu0",
        "colab_type": "text"
      },
      "source": [
        "Importamos la librerías y cargamos los datos de nuestro drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xhYDvAoh_oe",
        "colab_type": "code",
        "outputId": "c3e12b1f-aed5-4651-9820-ed408c926d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import pandas\n",
        "import numpy\n",
        "\n",
        "\n",
        "products = pandas.read_csv('/content/drive/My Drive/amazon_2.csv', low_memory=False, sep=',')\n",
        "products = products.fillna({'name':''})  # fill in N/A's in the review column\n",
        "products = products.fillna({'review':''})  # fill in N/A's in the review column\n",
        "print(products.dtypes)\n",
        "products.review=products.review.astype(str)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name      object\n",
            "review    object\n",
            "rating     int64\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PapvCXhLiiAO",
        "colab_type": "text"
      },
      "source": [
        "Operaciones:\n",
        "* Eliminar signos de puntuación\n",
        "* Eliminar reviews neutrales (rating 3).\n",
        "* Definir reviews con rating >=4 como positivas (+1) y reviews con rating <= 2 negativas (-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bukrp75SicE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punctuation(text):\n",
        "    import string\n",
        "    replace_punctuation=str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "    return text.translate(replace_punctuation) \n",
        "\n",
        "products['review_clean'] = products['review'].apply(remove_punctuation)\n",
        "products['review_clean'] = products['review_clean'].map(lambda x: x.lower())\n",
        "\n",
        "# Eliminar reviews neutrales\n",
        "products = products[products['rating'] != 3]\n",
        "# Sentimiento positivo +1 y sentimiento negativo ‐1\n",
        "products['sentiment'] = products['rating'].apply(lambda rating : +1 if rating > 3 else -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlUzcIxAjCwQ",
        "colab_type": "text"
      },
      "source": [
        "Dividimos los datos en train y test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRbhJF3Gi944",
        "colab_type": "code",
        "outputId": "27636ae7-f5f7-408e-97a3-9eade31d5d08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "numpy.random.seed(1) \n",
        "msk = numpy.random.rand(len(products)) < 0.8\n",
        "train_data = products[msk]\n",
        "test_data = products[~msk]\n",
        "print (len(train_data))\n",
        "print (len(test_data))    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42497\n",
            "10575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MWNO27kjPPo",
        "colab_type": "text"
      },
      "source": [
        "Entrenamiento de un clasificador logístico\n",
        "\n",
        "significant_words = ['love', 'great', 'easy', 'old', 'little', 'perfect', 'loves', \n",
        "      'well', 'able', 'car', 'broke', 'less', 'even', 'waste', 'disappointed', \n",
        "       'work', 'product', 'money', 'would', 'return']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewPVmitPjIQw",
        "colab_type": "code",
        "outputId": "837fb886-9e59-4d8f-fc88-04bed001324e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
        "# Representación de documentos como bolsa de palabras\n",
        "#vectorizer = CountVectorizer(vocabulary=significant_words) # limit to 20 words\n",
        "\n",
        "# Representacion de documentos de training a partir de la bolsa de palabras\n",
        "train_matrix = vectorizer.fit_transform(train_data['review_clean'])\n",
        "# Representacion de documentos de test a partir de la bolsa de palabras\n",
        "test_matrix = vectorizer.transform(test_data['review_clean'])\n",
        "\n",
        "# Entrenamiento de clasificador logístico\n",
        "from sklearn import linear_model\n",
        "model = linear_model.LogisticRegression()\n",
        "model.fit(train_matrix, train_data['sentiment'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un3ErG6IjgRp",
        "colab_type": "text"
      },
      "source": [
        "Métricas del clasificador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY-SUlkJjZhN",
        "colab_type": "code",
        "outputId": "25587ab6-ca6b-4c44-f6bc-e7a94cd556d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_true=test_data['sentiment'], y_pred=model.predict(test_matrix))\n",
        "print (\"Test Accuracy: %s\" % accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.8948463356973996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWDJGX5ZjnLM",
        "colab_type": "text"
      },
      "source": [
        "Baseline: Predicción de la clase mayoritaria"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBHp0rKUjfL8",
        "colab_type": "code",
        "outputId": "fa65d5f7-1086-499a-b592-5d36d28eec25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "baseline = len(test_data[test_data['sentiment'] == 1])/float(len(test_data))\n",
        "print (\"Baseline accuracy (majority class classifier): %s\" % baseline)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline accuracy (majority class classifier): 0.5015602836879433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FVyiMBSjq8b",
        "colab_type": "text"
      },
      "source": [
        "Matriz de confunsión"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUdiUNS7jwMn",
        "colab_type": "code",
        "outputId": "27643740-11c0-4b26-b923-052c749da679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cmat = confusion_matrix(y_true=test_data['sentiment'],y_pred=model.predict(test_matrix),labels=model.classes_) # use the same order of class as the LR model.\n",
        "print (' target_label | predicted_label | count ')\n",
        "print ('‐‐‐‐‐‐‐‐‐‐‐‐‐‐+‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐+‐‐‐‐‐‐‐')\n",
        "# Imprimir matriz de confusión\n",
        "for i, target_label in enumerate(model.classes_):\n",
        " for j, predicted_label in enumerate(model.classes_):\n",
        "  print ('{0:^13} | {1:^15} | {2:5d}'.format(target_label, predicted_label, cmat[i,j]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " target_label | predicted_label | count \n",
            "‐‐‐‐‐‐‐‐‐‐‐‐‐‐+‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐+‐‐‐‐‐‐‐\n",
            "     -1       |       -1        |  4718\n",
            "     -1       |        1        |   553\n",
            "      1       |       -1        |   559\n",
            "      1       |        1        |  4745\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fXEEX64j2U8",
        "colab_type": "text"
      },
      "source": [
        "Otras métricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycfs0VNWj4lt",
        "colab_type": "code",
        "outputId": "3ad82f5f-8d48-40fa-cd65-b2636a34862b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "precision = precision_score(y_true=test_data['sentiment'],y_pred=model.predict(test_matrix))\n",
        "print (\"Precision de datos de test: %s\" % precision)\n",
        "\n",
        "\n",
        "from sklearn.metrics import recall_score\n",
        "recall = recall_score(y_true=test_data['sentiment'],y_pred=model.predict(test_matrix))\n",
        "print (\"Recall de datos de test: %s\" % recall)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision de datos de test: 0.8956209890524727\n",
            "Recall de datos de test: 0.8946078431372549\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLAabUt7j5rb",
        "colab_type": "text"
      },
      "source": [
        "Precision recall tradeoff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NWp0NgDj8Xk",
        "colab_type": "code",
        "outputId": "19755487-9435-44a4-a647-fa0f442116f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Varying the threshold\n",
        "\n",
        "def apply_threshold(probabilities, threshold):\n",
        "### YOUR CODE GOES HERE\n",
        "# +1 if >= threshold and ‐1 otherwise.\n",
        "  return [1 if x >= threshold else -1 for x in probabilities]\n",
        "\n",
        "probabilities = model.predict_proba(test_matrix)[:,1]\n",
        "predictions_with_default_threshold = apply_threshold(probabilities, 0.5)\n",
        "predictions_with_high_threshold = apply_threshold(probabilities, 0.9)\n",
        "\n",
        "print (\"Number of positive predicted reviews (threshold = 0.5): %s\" % \\\n",
        "(sum([x for x in predictions_with_default_threshold if x == 1])))\n",
        "\n",
        "print (\"Number of positive predicted reviews (threshold = 0.9): %s\" % \\\n",
        "(sum([x for x in predictions_with_high_threshold if x == 1])))\n",
        "\n",
        "\n",
        "precision_with_default_threshold = precision_score(y_true=test_data['sentiment'],y_pred=predictions_with_default_threshold)\n",
        "recall_with_default_threshold = recall_score(y_true=test_data['sentiment'],y_pred=predictions_with_default_threshold)\n",
        "precision_with_high_threshold = precision_score(y_true=test_data['sentiment'],y_pred=predictions_with_high_threshold)\n",
        "recall_with_high_threshold = recall_score(y_true=test_data['sentiment'],y_pred=predictions_with_high_threshold)\n",
        "\n",
        "print (\"Precision (threshold = 0.5): %s\" % precision_with_default_threshold)\n",
        "print (\"Recall (threshold = 0.5) : %s\" % recall_with_default_threshold)\n",
        "\n",
        "print (\"Precision (threshold = 0.9): %s\" % precision_with_high_threshold)\n",
        "print (\"Recall (threshold = 0.9) : %s\" % recall_with_high_threshold)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positive predicted reviews (threshold = 0.5): 5298\n",
            "Number of positive predicted reviews (threshold = 0.9): 3964\n",
            "Precision (threshold = 0.5): 0.8956209890524727\n",
            "Recall (threshold = 0.5) : 0.8946078431372549\n",
            "Precision (threshold = 0.9): 0.9548435923309788\n",
            "Recall (threshold = 0.9) : 0.7136123680241327\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPVmqMDQkBpD",
        "colab_type": "text"
      },
      "source": [
        "Curva presición y recall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBDmbJ_MkDvV",
        "colab_type": "code",
        "outputId": "9278a68c-a8c7-4b70-c566-2d31bcb75a3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "threshold_values = numpy.linspace(0.5, 0.999999, num=100)\n",
        "precision_all = []\n",
        "recall_all = []\n",
        "\n",
        "\n",
        "for threshold in threshold_values:\n",
        "    predictions = apply_threshold(probabilities, threshold)\n",
        "    precision = precision_score(y_true=test_data['sentiment'],y_pred=predictions)\n",
        "    recall = recall_score(y_true=test_data['sentiment'],y_pred=predictions)\n",
        "    print ('Metrics Threshold %s Precision %s Recall %s' % (threshold, precision, recall))  \n",
        "    precision_all.append(precision)\n",
        "    recall_all.append(recall)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_pr_curve(precision, recall, title):\n",
        "    plt.rcParams['figure.figsize'] = 7, 5\n",
        "    plt.locator_params(axis = 'x', nbins = 5)\n",
        "    plt.plot(precision, recall, 'b-', linewidth=4.0, color = '#B0017F')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Precision')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.xlim(0.94, 1.0)   #0.78\n",
        "    plt.ylim(0.0, 1.0)\n",
        "    plt.rcParams.update({'font.size': 16})\n",
        "\n",
        "plot_pr_curve(precision_all, recall_all, 'Precision recall curve (all)')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Metrics Threshold 0.5 Precision 0.8956209890524727 Recall 0.8946078431372549\n",
            "Metrics Threshold 0.5050504949494949 Precision 0.8986834573554665 Recall 0.8880090497737556\n",
            "Metrics Threshold 0.5101009898989899 Precision 0.8990632766201491 Recall 0.8866892911010558\n",
            "Metrics Threshold 0.5151514848484848 Precision 0.9001150306748467 Recall 0.8851809954751131\n",
            "Metrics Threshold 0.5202019797979798 Precision 0.9008645533141211 Recall 0.8840497737556561\n",
            "Metrics Threshold 0.5252524747474747 Precision 0.9016172506738545 Recall 0.8829185520361991\n",
            "Metrics Threshold 0.5303029696969697 Precision 0.9026651216685979 Recall 0.8812217194570136\n",
            "Metrics Threshold 0.5353534646464646 Precision 0.9040247678018576 Recall 0.8808446455505279\n",
            "Metrics Threshold 0.5404039595959595 Precision 0.9051590380139644 Recall 0.8799019607843137\n",
            "Metrics Threshold 0.5454544545454545 Precision 0.9054552514074937 Recall 0.8793363499245852\n",
            "Metrics Threshold 0.5505049494949494 Precision 0.9059945504087193 Recall 0.8776395173453997\n",
            "Metrics Threshold 0.5555554444444444 Precision 0.907353228008582 Recall 0.8770739064856712\n",
            "Metrics Threshold 0.5606059393939394 Precision 0.9075449569976545 Recall 0.8753770739064857\n",
            "Metrics Threshold 0.5656564343434344 Precision 0.9085210577864838 Recall 0.8744343891402715\n",
            "Metrics Threshold 0.5707069292929293 Precision 0.9096976835492736 Recall 0.8736802413273002\n",
            "Metrics Threshold 0.5757574242424243 Precision 0.9101454974439638 Recall 0.872737556561086\n",
            "Metrics Threshold 0.5808079191919192 Precision 0.9106685071977914 Recall 0.8706636500754148\n",
            "Metrics Threshold 0.5858584141414142 Precision 0.9112998814697748 Recall 0.8697209653092006\n",
            "Metrics Threshold 0.5909089090909091 Precision 0.912621359223301 Recall 0.8684012066365008\n",
            "Metrics Threshold 0.595959404040404 Precision 0.9128795395911887 Recall 0.8672699849170438\n",
            "Metrics Threshold 0.601009898989899 Precision 0.9133545310015898 Recall 0.8665158371040724\n",
            "Metrics Threshold 0.6060603939393939 Precision 0.9139613622784306 Recall 0.8651960784313726\n",
            "Metrics Threshold 0.6111108888888889 Precision 0.9158504897061763 Recall 0.8638763197586727\n",
            "Metrics Threshold 0.6161613838383838 Precision 0.9161496898138883 Recall 0.8631221719457014\n",
            "Metrics Threshold 0.6212118787878788 Precision 0.9167502507522568 Recall 0.8616138763197587\n",
            "Metrics Threshold 0.6262623737373737 Precision 0.9175547958978484 Recall 0.8602941176470589\n",
            "Metrics Threshold 0.6313128686868686 Precision 0.9180261832829809 Recall 0.8593514328808446\n",
            "Metrics Threshold 0.6363633636363636 Precision 0.9182973572725439 Recall 0.8582202111613876\n",
            "Metrics Threshold 0.6414138585858585 Precision 0.918989898989899 Recall 0.8576546003016591\n",
            "Metrics Threshold 0.6464643535353535 Precision 0.9194495041489577 Recall 0.8565233785822021\n",
            "Metrics Threshold 0.6515148484848485 Precision 0.9207800121876905 Recall 0.8546380090497737\n",
            "Metrics Threshold 0.6565653434343435 Precision 0.9215726217152169 Recall 0.8529411764705882\n",
            "Metrics Threshold 0.6616158383838384 Precision 0.9221177432542927 Recall 0.8504901960784313\n",
            "Metrics Threshold 0.6666663333333334 Precision 0.9227617291538619 Recall 0.8491704374057315\n",
            "Metrics Threshold 0.6717168282828283 Precision 0.9237723443599754 Recall 0.8476621417797888\n",
            "Metrics Threshold 0.6767673232323232 Precision 0.9245827323305172 Recall 0.8459653092006033\n",
            "Metrics Threshold 0.6818178181818182 Precision 0.9258111179995867 Recall 0.8446455505279035\n",
            "Metrics Threshold 0.6868683131313131 Precision 0.9269254722856549 Recall 0.8418174962292609\n",
            "Metrics Threshold 0.6919188080808081 Precision 0.9273824386183936 Recall 0.8403092006033183\n",
            "Metrics Threshold 0.696969303030303 Precision 0.9292275574112735 Recall 0.8391779788838613\n",
            "Metrics Threshold 0.702019797979798 Precision 0.9299016530654949 Recall 0.8378582202111614\n",
            "Metrics Threshold 0.7070702929292929 Precision 0.930607966457023 Recall 0.8369155354449472\n",
            "Metrics Threshold 0.7121207878787879 Precision 0.9310634720470786 Recall 0.8352187028657617\n",
            "Metrics Threshold 0.7171712828282828 Precision 0.9313395113732098 Recall 0.833710407239819\n",
            "Metrics Threshold 0.7222217777777777 Precision 0.9324181626187962 Recall 0.8323906485671192\n",
            "Metrics Threshold 0.7272722727272727 Precision 0.9328105129292072 Recall 0.8297511312217195\n",
            "Metrics Threshold 0.7323227676767676 Precision 0.933064173395665 Recall 0.827865761689291\n",
            "Metrics Threshold 0.7373732626262626 Precision 0.9332337883959044 Recall 0.8248491704374057\n",
            "Metrics Threshold 0.7424237575757575 Precision 0.934475374732334 Recall 0.8227752639517345\n",
            "Metrics Threshold 0.7474742525252525 Precision 0.9344227047946678 Recall 0.8193815987933635\n",
            "Metrics Threshold 0.7525247474747474 Precision 0.934899762879931 Recall 0.817684766214178\n",
            "Metrics Threshold 0.7575752424242423 Precision 0.9363774074875568 Recall 0.8157993966817496\n",
            "Metrics Threshold 0.7626257373737373 Precision 0.936917407327119 Recall 0.8148567119155354\n",
            "Metrics Threshold 0.7676762323232322 Precision 0.9372012168622338 Recall 0.8131598793363499\n",
            "Metrics Threshold 0.7727267272727272 Precision 0.9372138652714193 Recall 0.8105203619909502\n",
            "Metrics Threshold 0.7777772222222221 Precision 0.9383067162546489 Recall 0.8086349924585219\n",
            "Metrics Threshold 0.7828277171717171 Precision 0.9383095499451153 Recall 0.8058069381598794\n",
            "Metrics Threshold 0.787878212121212 Precision 0.9393337745422458 Recall 0.802790346907994\n",
            "Metrics Threshold 0.792928707070707 Precision 0.9405633178088267 Recall 0.7995852187028658\n",
            "Metrics Threshold 0.797979202020202 Precision 0.9416090929351459 Recall 0.7965686274509803\n",
            "Metrics Threshold 0.803029696969697 Precision 0.9416778824585016 Recall 0.7914781297134238\n",
            "Metrics Threshold 0.8080801919191919 Precision 0.9429537767756483 Recall 0.7884615384615384\n",
            "Metrics Threshold 0.8131306868686868 Precision 0.9436268960833145 Recall 0.7858220211161387\n",
            "Metrics Threshold 0.8181811818181818 Precision 0.9442675159235668 Recall 0.7826168929110106\n",
            "Metrics Threshold 0.8232316767676767 Precision 0.9453214367421643 Recall 0.7790346907993967\n",
            "Metrics Threshold 0.8282821717171717 Precision 0.9456971928209849 Recall 0.7748868778280543\n",
            "Metrics Threshold 0.8333326666666666 Precision 0.9463459759481961 Recall 0.7714932126696833\n",
            "Metrics Threshold 0.8383831616161616 Precision 0.9468060394889664 Recall 0.7684766214177979\n",
            "Metrics Threshold 0.8434336565656565 Precision 0.9478971962616822 Recall 0.764894419306184\n",
            "Metrics Threshold 0.8484841515151514 Precision 0.9487179487179487 Recall 0.7603695324283559\n",
            "Metrics Threshold 0.8535346464646465 Precision 0.949692961738309 Recall 0.7581070889894419\n",
            "Metrics Threshold 0.8585851414141414 Precision 0.9501069137562367 Recall 0.7539592760180995\n",
            "Metrics Threshold 0.8636356363636364 Precision 0.9502273271117492 Recall 0.7486802413273002\n",
            "Metrics Threshold 0.8686861313131313 Precision 0.9503853564547207 Recall 0.7439668174962293\n",
            "Metrics Threshold 0.8737366262626263 Precision 0.9506172839506173 Recall 0.7403846153846154\n",
            "Metrics Threshold 0.8787871212121212 Precision 0.9514515735545255 Recall 0.7352941176470589\n",
            "Metrics Threshold 0.8838376161616162 Precision 0.9521824423737126 Recall 0.7320889894419306\n",
            "Metrics Threshold 0.8888881111111111 Precision 0.9522158950235207 Recall 0.7251131221719457\n",
            "Metrics Threshold 0.893938606060606 Precision 0.9533782099227125 Recall 0.7209653092006033\n",
            "Metrics Threshold 0.898989101010101 Precision 0.9546941857538385 Recall 0.7151206636500754\n",
            "Metrics Threshold 0.9040395959595959 Precision 0.9559010960999236 Recall 0.7070135746606335\n",
            "Metrics Threshold 0.9090900909090909 Precision 0.9577210621294148 Recall 0.7004147812971342\n",
            "Metrics Threshold 0.9141405858585858 Precision 0.9587897756911842 Recall 0.6930618401206636\n",
            "Metrics Threshold 0.9191910808080808 Precision 0.9590705043570108 Recall 0.6847662141779789\n",
            "Metrics Threshold 0.9242415757575757 Precision 0.9595065701260391 Recall 0.6745852187028658\n",
            "Metrics Threshold 0.9292920707070706 Precision 0.9611729568286723 Recall 0.667420814479638\n",
            "Metrics Threshold 0.9343425656565656 Precision 0.9630955659597907 Recall 0.6593137254901961\n",
            "Metrics Threshold 0.9393930606060605 Precision 0.9641958041958042 Recall 0.6498868778280543\n",
            "Metrics Threshold 0.9444435555555555 Precision 0.9661547212741752 Recall 0.6404600301659125\n",
            "Metrics Threshold 0.9494940505050504 Precision 0.9667052692530399 Recall 0.629524886877828\n",
            "Metrics Threshold 0.9545445454545454 Precision 0.9669128508124076 Recall 0.6170814479638009\n",
            "Metrics Threshold 0.9595950404040403 Precision 0.9687689508793208 Recall 0.6023755656108597\n",
            "Metrics Threshold 0.9646455353535353 Precision 0.9702380952380952 Recall 0.5838989441930619\n",
            "Metrics Threshold 0.9696960303030302 Precision 0.9719960924780202 Recall 0.5627828054298643\n",
            "Metrics Threshold 0.9747465252525251 Precision 0.9741760108732586 Recall 0.5405354449472096\n",
            "Metrics Threshold 0.9797970202020201 Precision 0.9745519713261649 Recall 0.5126319758672699\n",
            "Metrics Threshold 0.984847515151515 Precision 0.9771936606107461 Recall 0.4766214177978884\n",
            "Metrics Threshold 0.98989801010101 Precision 0.9801124081279723 Recall 0.4274132730015083\n",
            "Metrics Threshold 0.9949485050505049 Precision 0.9841017488076311 Recall 0.3501131221719457\n",
            "Metrics Threshold 0.999999 Precision 0.9819277108433735 Recall 0.0307315233785822\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcQAAAFNCAYAAACe4B8PAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 504x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}